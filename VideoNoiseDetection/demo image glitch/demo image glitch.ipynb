{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51b7326f-e9c5-4f50-b271-9f8fb574d262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from glitch_this import ImageGlitcher\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchinfo import summary\n",
    "import os\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f5c35b2-908e-489c-bc0d-0d44f0d54b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(path):\n",
    "    images_list = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for name in files:\n",
    "            images_list.append(os.path.join(root, name))\n",
    "\n",
    "    return images_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a917843-1fa7-45df-abab-ed143afdcc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root, target, transform=None):\n",
    "        self.root = root\n",
    "        self.images = get_images(root)\n",
    "        self.targets = [int(target) for i in range(len(self.images))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.images[index]  \n",
    "        image = PIL.Image.open(image_name).convert('RGB')\n",
    "        label = torch.tensor(self.targets[index],dtype=torch.float32)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return (image, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23a7da8b-f14c-40bb-a774-fa4d7a517d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixelate(img):\n",
    "    # Resize down to 128x128 pixels\n",
    "    imgSmall = img.resize((128,128), resample=PIL.Image.Resampling.BILINEAR)\n",
    "    \n",
    "    # Scale back up using NEAREST to original size\n",
    "    return imgSmall.resize(img.size, PIL.Image.Resampling.NEAREST)\n",
    "\n",
    "def shuffle_pixel(region):\n",
    "    temp = np.reshape(region, (-1, region.shape[2]))\n",
    "    np.random.shuffle(temp)\n",
    "    region = np.reshape(temp, region.shape)\n",
    "    return region\n",
    "\n",
    "def distort(img):\n",
    "    img = np.array(img)\n",
    "    \n",
    "    for i in range(img.shape[0]//14):\n",
    "        for j in range(img.shape[1]//14):\n",
    "            if np.random.randint(0,5) == 0:\n",
    "                img[14*i:14*i+14,14*j:14*j+14,:] = shuffle_pixel(img[14*i:14*i+14,14*j:14*j+14,:])\n",
    "\n",
    "    return PIL.Image.fromarray(np.uint8(img)).convert('RGB')\n",
    "            \n",
    "glitcher = ImageGlitcher()\n",
    "\n",
    "def glitch(img):\n",
    "    glitch_amount = np.random.randint(2,9)\n",
    "    if np.random.randint(0,1) == 0:\n",
    "        return glitcher.glitch_image(img, glitch_amount, color_offset=False)\n",
    "    else:\n",
    "        return glitcher.glitch_image(img, glitch_amount, color_offset=True)\n",
    "\n",
    "def noise_lambda(img):\n",
    "    if np.random.randint(0,2) == 0:\n",
    "        return pixelate(distort(img))\n",
    "    else:\n",
    "        return glitch(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a9dd092-b862-4914-b55e-7ae810979dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_black_border(image):\n",
    "    y_nonzero, _, _ = np.nonzero(np.array(image) > 10)\n",
    "    return image.crop((0, np.min(y_nonzero), image.size[0], np.max(y_nonzero)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "946f61c0-f1bd-4535-a731-279ef710ff35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 1, 1, 2, 2]), array([0, 1, 0, 1, 0, 1]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[3, 1], [1,2], [5, 6]])\n",
    "np.nonzero(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2bb5089-6d08-4ec5-8c1f-132f6bb897e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# caltech dataset\n",
    "# root = \"../caltech101/101_ObjectCategories/\"\n",
    "\n",
    "# UCF101 dataset\n",
    "root = \"../Data/505_video_frames/\"\n",
    "batch_size=32\n",
    "\n",
    "normal_transform = transforms.Compose([\n",
    "        transforms.Lambda(crop_black_border),\n",
    "        transforms.Resize((224,224)),           \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "glitch_transform = transforms.Compose([ \n",
    "        transforms.Lambda(crop_black_border),\n",
    "        transforms.Resize((224,224)),  \n",
    "        transforms.Lambda(noise_lambda),  \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e7e9c0c-6557-42bf-a719-ea492288452b",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset = CustomDataset(root=root,target=0,transform=normal_transform)\n",
    "glitch_dataset = CustomDataset(root=root,target=1,transform=glitch_transform)\n",
    "dataset = ConcatDataset([original_dataset, glitch_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee987a00-5b51-4df1-9c07-286dd3412fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30d7c051-1dcd-4bdc-a47b-4e113c71c89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d953bcc8-b53a-4e2a-8c13-c62a6e98144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlitchDetect(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3,16,3,1)\n",
    "        self.conv2 = nn.Conv2d(16,32,3,1)\n",
    "        self.conv3 = nn.Conv2d(32,64,3,1)\n",
    "        self.conv4 = nn.Conv2d(64,128,3,1)\n",
    "        self.conv5 = nn.Conv2d(128,256,3,1)\n",
    "        self.fc1 = nn.Linear(6400,2056)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(2056,512)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(512,1)  \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x,2,2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x,2,2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x,2,2)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x,2,2)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.max_pool2d(x,2,2)\n",
    "\n",
    "        x = x.view(-1,6400)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1f624c0-5892-4cee-a904-4ba817f680cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GlitchDetect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbfb7765-f641-4b77-a616-a6ea89c2159d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "GlitchDetect                             [32, 1]                   --\n",
      "├─Conv2d: 1-1                            [32, 16, 222, 222]        448\n",
      "├─Conv2d: 1-2                            [32, 32, 109, 109]        4,640\n",
      "├─Conv2d: 1-3                            [32, 64, 52, 52]          18,496\n",
      "├─Conv2d: 1-4                            [32, 128, 24, 24]         73,856\n",
      "├─Conv2d: 1-5                            [32, 256, 10, 10]         295,168\n",
      "├─Linear: 1-6                            [32, 2056]                13,160,456\n",
      "├─Dropout: 1-7                           [32, 2056]                --\n",
      "├─Linear: 1-8                            [32, 512]                 1,053,184\n",
      "├─Dropout: 1-9                           [32, 512]                 --\n",
      "├─Linear: 1-10                           [32, 1]                   513\n",
      "==========================================================================================\n",
      "Total params: 14,606,761\n",
      "Trainable params: 14,606,761\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 6.83\n",
      "==========================================================================================\n",
      "Input size (MB): 19.27\n",
      "Forward/backward pass size (MB): 369.58\n",
      "Params size (MB): 58.43\n",
      "Estimated Total Size (MB): 447.28\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(summary(model, input_size=(batch_size, 3, 224, 224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32cf2df4-927e-4e6b-9da4-8167021e0f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=2, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss and abs(validation_loss - self.min_validation_loss) > self.min_delta:\n",
    "            self.counter = 0\n",
    "            self.min_validation_loss = validation_loss\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15cb4d01-17e8-47ca-acc3-3a5080282656",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.00001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min',factor=0.1,patience=2,verbose=True)\n",
    "early_stopper = EarlyStopper(patience=3, min_delta=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "247ace69-411f-4a96-a365-a5d31eeaac50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     432\n",
      "      16\n",
      "    4608\n",
      "      32\n",
      "   18432\n",
      "      64\n",
      "   73728\n",
      "     128\n",
      "  294912\n",
      "     256\n",
      "13158400\n",
      "    2056\n",
      " 1052672\n",
      "     512\n",
      "     512\n",
      "       1\n",
      "________\n",
      "14606761\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>8}')\n",
    "    print(f'________\\n{sum(params):>8}')\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41f3eee6-6424-46c8-af85-8467fefdbe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6c0eed-3489-4a78-8819-a588e1909e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [04:05,  4.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  batch: 1000 [ 32000/127947]  loss: 0.7071014046669006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [07:59,  4.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  batch: 2000 [ 64000/127947]  loss: 0.6153396368026733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [11:59,  4.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  batch: 3000 [ 96000/127947]  loss: 0.7005798816680908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3999it [15:55,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  finished,  validation loss: 0.7158223390579224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "793it [02:59,  4.91it/s]"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    for b, (X_train, y_train) in tqdm(enumerate(train_loader)):\n",
    "        b+=1\n",
    "        model.train()\n",
    "        X_train = X_train.to(device='cuda')\n",
    "        y_train = y_train.to(device='cuda')\n",
    "        pred = model(X_train)\n",
    "        loss = criterion(pred, y_train.unsqueeze(dim=-1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if b % 1000 == 0:\n",
    "            print(f'epoch: {i:2}  batch: {b:4} [{batch_size*b:6}/{len(train_dataset)}]  loss: {loss.item()}')\n",
    "\n",
    "    # train_losses.append(loss)\n",
    "\n",
    "    running_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for vb, (X_test, y_test) in enumerate(test_loader):\n",
    "            X_test = X_test.to(device='cuda')\n",
    "            y_test = y_test.to(device='cuda')\n",
    "            val = model(X_test)\n",
    "            loss = criterion(val, y_test.unsqueeze(dim=-1))\n",
    "            running_loss += loss\n",
    "    # test_losses.append(loss)\n",
    "\n",
    "    avg_loss = running_loss / (vb+1)\n",
    "    print(f'epoch: {i:2}  finished,  validation loss: {loss.item()}')\n",
    "    if early_stopper.early_stop(loss):             \n",
    "        break\n",
    "    scheduler.step(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f1229e-7fd3-4e80-84ab-be50b1d5a8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    for X_test, y_test in test_loader:\n",
    "        X_test = X_test.to(device='cuda')\n",
    "        y_test = y_test.to(device='cuda')\n",
    "        y_val = model(X_test)\n",
    "        predicted = torch.round(y_val)\n",
    "        correct += (predicted == y_test.unsqueeze(dim=-1)).sum()\n",
    "print(f'Test accuracy: {correct.item()}/{len(test_dataset)} = {correct.item()*100/len(test_dataset):7.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4ef9ea-2d1b-4977-9833-59354eb4f3fa",
   "metadata": {},
   "source": [
    "# Test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931081c3-1ba9-4379-b874-757ed1e6fef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsample_path = './Capture.PNG'\n",
    "newsample = PIL.Image.open(newsample_path).convert('RGB')\n",
    "plt.imshow(newsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06c9b81-ff3b-4800-8c4f-3b8a1f7bf4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsample.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885c171f-15d1-459b-8834-dbfacc0be56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "    std=[1/0.229, 1/0.224, 1/0.225]\n",
    ")\n",
    "# im_inv = inv_normalize(glitch_dataset[4023][0])\n",
    "# plt.imshow(np.transpose(im_inv.numpy(), (1, 2, 0)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737d3a0e-7b1c-4548-ae63-e4568a44eb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = normal_transform(newsample)\n",
    "img_inv = inv_normalize(img)\n",
    "plt.imshow(np.transpose(img_inv.numpy(), (1, 2, 0)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbbee52-922c-47bc-968d-e42f40adbf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "y_val = model(img.to(device='cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d66e30-ed89-4ecb-b5d1-4421b6e3ddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8d7175-a109-44e8-af50-f49269d3dcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_inv = inv_normalize(train_dataset[6][0])\n",
    "plt.imshow(np.transpose(img_inv.numpy(), (1, 2, 0)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fc07b5-5dc6-4859-970d-08a935b2880d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
